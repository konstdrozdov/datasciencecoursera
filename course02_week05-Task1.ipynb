{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.1.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.12 (default, Nov 19 2016 06:48:10)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.enableHiveSupport().master(\"local [2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sparkSession.read.parquet(\"/data/sample264\")\n",
    "meta = sparkSession.read.parquet(\"/data/meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization could be done by next function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, sum\n",
    "\n",
    "def norm(df, key1, key2, field, n): \n",
    "    \n",
    "    window = Window.partitionBy(key1).orderBy(col(field).desc())\n",
    "        \n",
    "    topsDF = df.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= n) \\\n",
    "        .drop(col(\"row_number\")) \n",
    "        \n",
    "    tmpDF = topsDF.groupBy(col(key1)).agg(col(key1), sum(col(field)).alias(\"sum_\" + field))\n",
    "   \n",
    "    normalizedDF = topsDF.join(tmpDF, key1, \"inner\") \\\n",
    "        .withColumn(\"norm_\" + field, col(field) / col(\"sum_\" + field)) \\\n",
    "        .cache()\n",
    "\n",
    "    return normalizedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, rank\n",
    "\n",
    "userTrack = data.groupBy(col(\"userId\"), col(\"trackId\")).count()\n",
    "\n",
    "userTrackNorm = norm(userTrack, \"userId\", \"trackId\", \"count\", 1000) \\\n",
    "        .withColumn(\"id\", col(\"userId\")) \\\n",
    "        .withColumn(\"id2\", col(\"trackId\")) \\\n",
    "        .withColumn(\"norm_count\", col(\"norm_count\") * 0.5) \\\n",
    "        .select(col(\"id\"), col(\"id2\"), col(\"norm_count\"))     \n",
    "\n",
    "window = Window.orderBy(col(\"norm_count\"))\n",
    "    \n",
    "userTrackList = userTrackNorm.withColumn(\"position\", rank().over(window))\\\n",
    "    .filter(col(\"position\") < 50)\\\n",
    "    .orderBy(col(\"id\"), col(\"id2\"))\\\n",
    "    .select(col(\"id\"), col(\"id2\"))\\\n",
    "    .take(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc, asc, collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+\n",
      "|   id1|   id2|count|\n",
      "+------+------+-----+\n",
      "|870292|939606|  253|\n",
      "|939606|870292|  253|\n",
      "|854531|879259|  195|\n",
      "|879259|854531|  195|\n",
      "|933030|871513|  159|\n",
      "+------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SEVEN_MIN = 60 * 7\n",
    "\n",
    "tracks_tracks = data \\\n",
    "    .alias('data_1').join(data.alias('data_2'), \\\n",
    "        (col('data_1.userId') == col('data_2.userId')) &\n",
    "        (col('data_1.trackId') != col('data_2.trackId')) &\n",
    "        (\n",
    "            (col('data_1.timestamp') - col('data_2.timestamp') <= SEVEN_MIN) &\n",
    "            (col('data_1.timestamp') - col('data_2.timestamp') >= -SEVEN_MIN)\n",
    "        ), 'inner') \\\n",
    "    .select(\n",
    "        col('data_1.trackId').alias('id1'),\n",
    "        col('data_2.trackId').alias('id2')\n",
    "    ) \\\n",
    "    .groupBy(col('id1'), col('id2')).count() \\\n",
    "    .orderBy(desc('count'))\n",
    "\n",
    "tracks_tracks.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+\n",
      "|   id1|   id2|norm_count|\n",
      "+------+------+----------+\n",
      "|798256|923706|       1.0|\n",
      "|798319|837992|       1.0|\n",
      "|798322|876562|       1.0|\n",
      "|798331|827364|       1.0|\n",
      "|798335|840741|       1.0|\n",
      "+------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = norm(tracks_tracks, 'id1', 'id2', 'count', 40) \\\n",
    "    .select(col('id1'), col('id2'), col('norm_count')) \\\n",
    "    .orderBy(desc('norm_count'), asc('id1'), asc('id2'))\n",
    "\n",
    "results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798256 923706\n",
      "798319 837992\n",
      "798322 876562\n",
      "798331 827364\n",
      "798335 840741\n",
      "798374 816874\n",
      "798375 810685\n",
      "798379 812055\n",
      "798380 840113\n",
      "798396 817687\n",
      "798398 926302\n",
      "798405 867217\n",
      "798443 905923\n",
      "798457 918918\n",
      "798460 891840\n",
      "798461 940379\n",
      "798470 840814\n",
      "798474 963162\n",
      "798477 883244\n",
      "798485 955521\n",
      "798505 905671\n",
      "798545 949238\n",
      "798550 936295\n",
      "798626 845438\n",
      "798691 818279\n",
      "798692 898823\n",
      "798702 811440\n",
      "798704 937570\n",
      "798725 933147\n",
      "798738 894170\n",
      "798745 799665\n",
      "798782 956938\n",
      "798801 950802\n",
      "798820 890393\n",
      "798833 916319\n",
      "798865 962662\n",
      "798931 893574\n",
      "798946 946408\n",
      "799012 809997\n",
      "799024 935246\n"
     ]
    }
   ],
   "source": [
    "for row in results.take(40):\n",
    "    print row['id1'], row['id2']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
